\documentclass{homework}
\author{Tomás Pérez}
\class{Density Functional Theory - Lecture Notes}
\date{\today}
\title{Theory \& Notes}

\graphicspath{{./media/}}

\begin{document} \maketitle

\section{Quantum Many-Body Problem}

In this section, we're interested in the non-relativistic quantum many body problem, where the system of study is made up of electrons and nuclei. The theory's Hamiltonian is given by

\begin{align}
    \hat{\bf H} = -\frac{\hbar^2}{2m_e} \sum_i \nabla^2_i - \sum_{i, I} \frac{Z_I e^2}{|{\bf x}_i - {\bf R}_I|} + \frac{1}{2} \sum_{i \neq j} \frac{e^2}{|{\bf x}_i-{\bf x}_j|} - \sum_{I} \frac{\hbar^2}{2M_I} \nabla^2_I + \frac{1}{2} \sum_{I \neq J} \frac{Z_I Z_J e^2}{|{\bf R}_I - {\bf R}_J|},
\label{many-body Hamiltonian}
\end{align}

where 

\begin{itemize}
    \item the first corresponds to the sum of the kinetic energy of each one of the electrons,
    \item the second term correspond to the Coulomb interaction created by the nuclei, where the position of $i$-th electron is ${\bf x}_i$ and the nuclei's position is ${\bf R}_I$,
    \item the third term is the Coulomb interaction between the electrons, where we exclude any auto-interaction of an electron with itself. Note that this term forces to consider more than one electron at a time. 
    \item The fourth term is the kinetic energy of the nuclei,
    \item and where the last term accounts for the nucleus-nucleus interaction. \\
\end{itemize}

Solving the general $n$-body problem consists then in finding the eigenfunctions and eigenvalues of the coupled, second order, differential equation given by the action of \eqref{many-body Hamiltonian} on the $n$-body wave function. However, by inspecting the terms of the Hamiltonian we can simplify the problem. Note that the mass of the nuclei is much larger than the mass of the electrons, therefore we can neglect their kinetic energy given by \eqref{many-body Hamiltonian}'s fourth term. Likewise, if we consider the position of the nuclei as parameters, the fifth term adds just a constant. Taking the previous points into consideration, yields a more simplified Hamiltonian:

\begin{equation}
    \hat{\bf H} = -\frac{\hbar^2}{2m_e} \sum_i \nabla^2_i - \sum_{i, I} \frac{Z_I e^2}{|{\bf x}_i - {\bf R}_I|} + \frac{1}{2} \sum_{i \neq j} \frac{e^2}{|{\bf x}_i-{\bf x}_j|} + \sum_{i} {\bf V}_i ({\bf x}_i).
\label{simplified many-body Hamiltonian}
\end{equation}

Note that in said equation, in the second term, we considered only the Coulomb interaction between the electrons and the nuclei. We might as well add any other external potentials acting on the electrons, including the ones stemming from the nuclei, which are accounted for in the fourth term. \\

This is still a highly-non trivial problem to solve. Indeed, solving said problem consists of solving the eigenvalue equation:

\begin{equation}
\hat{\bf H}({\bf x}_1, \cdots, {\bf x}_N) \Psi_\lambda(\tilde{x}_1, \cdots, \tilde{x}_N) = E_\lambda \Psi_\lambda(\tilde{x}_1, \cdots, \tilde{x}_N),
\label{ev equation}
\end{equation}

where 

\begin{itemize}
    \item $\tilde{x}_i = ({\bf x}_i, \sigma_i)$, is an atlas on a Riemannian manifold $V = \R^{3N} \otimes (\mathds{C}^2)^{\otimes N}$, accounting for both the $i$-th electron's position and spin,
    \item and where $\Psi_\lambda$, describing the wavefunction of the $N$-body system, is a function in a Hilbert space given by $\mathds{H} \approx L^2(\R^{3N}) \otimes \bigotimes_{j=1}^{N} \mathds{C}^2 \approx L^2(\R^{3N}) \otimes (\mathds{C}^{2})^{\otimes N}$\footnote{Remember that the Hilbert space for a particle in $\R^3$ with spin is the tensor product of $L^2(\R^3)$ with a finite-dimensional vector space $V$, where $V$ carries and irreducible action of the group rotation $SO(3)$. In this context, the proper notion of "action" is the projective representation of $SO(3)$, meaning a family of operators satisfying the SO(3)-Lie algebra, $\mathfrak{so}(3)$. In our context, the full Hilbert space of a system of $N$ spin-$s$ particles is isomorphic to the tensor product of $N$-Hilbert spaces $\mathds{C}^2$. This may be written in short as $(\mathds{C}^{2s+1})^{\otimes N}$}. Even if we ignore the spatial degrees of freedom, the spin-Hilbert space's dimension scales up as $2^N$. Thus, this problem becomes an NP-hard problem. This is even worse given that we cannot decouple the coordinates of the $i$-th electron from the $j$-th electron. \\ 
\end{itemize}

One way to simplify \eqref{ev equation} is by disregarding the Coulomb interaction and the Pauli principle, considering a system of non-interacting particles. This turns said eigenvalue equation into, a priori, $N$-single electron uncoupled problems, ie.

$$
\Psi_\lambda(\tilde{x}_1, \cdots, \tilde{x}_N) = \prod_{i=1}^{N} \psi_{n_i}(\tilde{x}_i), \textnormal{with } \psi_{n_i}(\tilde{x}) \in L^2(\R^{3}) \otimes \mathds{C}^2 \textnormal{ and } \lambda = \{n_i\}_{i=1}^{N}.
$$

This problem, while tractable, is unphysical since it implies that all electrons move independently in the external potential, allowing for multiple electrons to be in the fundamental state (or any other state for that matter), which goes against the Pauli principle. One way in which we can take into account the Pauli Principle is by taking the Slater determinant of the $N$ single-electron wave functions. Even so, the electrons would still be independent. \\

\section{The Hartree-Fock formalism}

Consider the non-relativistic Hamiltonian given by 

\begin{equation}
    \hat{\bf H} = -\frac{\hbar^2}{2m_e} \sum_i \nabla^2_i + \sum_{i} {\bf V}_{\textnormal{ext}} ({\bf x}_i) + \frac{1}{2} \sum_{i \neq j} \frac{e^2}{|{\bf x}_i-{\bf x}_j|},
\label{hamiltonian HF}
\end{equation}

which contains the kinetic energy of the $i$-th electron, the external potential, which acts as a one-body operator, and the Coulomb interaction. Hartree-Fock's approach consists on initially considering a system of almost independent particles, in which each electron moves in the electrostatic potential of all other electrons. This is, the $n$-body normalized wave-function is factorized in a product in a product of normalized, independent particle wavefunctions, ie.

\begin{align}
     & \Psi^{HF} (\x_1, \cdots, \x_n) = \prod_{\bm{\alpha} \in \Lambda} \psi_{\bm{\alpha}}(\x, {\bm \sigma}) =  \prod_{j \in \Lambda}\psi_{\alpha_j, s_j}(\x_j, \sigma_j),
     \label{Hartree's ansatz}
\end{align}

where $\bm{\alpha} \in \Lambda$ indexed all quantum numbers (eg. $nlms$ for an atom or $n{\bf k}s$ for a crystal). Further conditions are imposed on the one-body wavefunctions,

\begin{align}
     \psi_{\alpha_j, s_j}(\x_j, \sigma_j) =  \left\{ \begin{array}{lcc}
         \psi_{\alpha_j}(\x_j) \chi_{\frac{1}{2}} & \textnormal{ for } \sigma_j = +1 \\
         \psi_{\alpha_j}(\x_j) \chi_{-\frac{1}{2}} & \textnormal{ for } \sigma_j = -1   
     \end{array} \right. \textnormal{ and }
     \xmeasure \spinmeasure \psi_{\alpha_j, s_j}^{*}(\x_j, \sigma_j) \psi_{\alpha_j, s_j}(\x_j, \sigma_j) = 1,
     \label{HF normalization}
\end{align}

 Note that this ansatz cares not for wavefunction's antisymmetry, though it can still abide by Pauli's principle if no more than two electron are assigned for each wavefunction $\psi_{\alpha_j}(\x_j)$. \\
 
Then, a natural approximation to the $i$-th electron's dynamics, is to consider it under the influence of the average potential produced by the other $N-1$ electrons, so that the Schr\"odinger-like equation for the $i$-th electron reads as 

\begin{align}
    \bigg[-\frac{1}{2} \nabla^2 + {\bf V}_{\textnormal{ext}}(\x_i) + \sum_{\bm{\lambda} \in \Lambda, \blanky \bm{\lambda} \neq \bm{\alpha}} \int_{\R^3} d\x \frac{n_{\bm{\lambda}}(\x)}{|\x_i - \x|} \bigg] \psi_{\bm{\alpha}}(\x) = E_{\alpha_i}  \psi_{\bm{\alpha}}(\x), \\
    \begin{array}{c}
         \textnormal{which, can be }  \\
         \textnormal{rewritten as}
    \end{array}
    \bigg[-\frac{1}{2} \nabla^2 + {\bf V}_{\textnormal{ext}}(\x_i) + {\bf V}_{\textnormal{HF}}(\x_i) \bigg] \psi_{\bm{\alpha}}(\x) = E_{\bm{\alpha}} \psi_{\bm{\alpha}}(\x),
    \label{Hartree-Fock's equation}
\end{align}

where $n_{\bm{\alpha}} = |\psi_{\bm{\alpha}}(\x)|^2$, is the $\bm{\alpha}$-th electron's charge density. Said prescription thus yields \eqref{Hartree-Fock's equation}, Hartree-Fock's equations. These equations are to be solved in an iterative way, since the determination of the Hartree potential implies the knowledge of the very same wavefunctions of interest. A good starting point for the wavefunctions is represented by the atomic wavefunctions. The physical meaning of the eigenvalues, the eigenenergies, is given by the variational principle and by Koopmans' theorem. \\

It's also possible to derive Hartree-Fock's equations in a more systematic way, by the means of the variational principle and the Lagrange's multiplier method. \\

Consider the ansatz given by \eqref{Hartree's ansatz}, which is only correct for the case of independent particles. Here, the objective is not to describe independent particles, but rather interacting particles, within one \textit{best single-particle} independent particle Hamiltonian, to which the single particle wavefunctions obey. Then, a possible trial wavefunction would be that which minimizes the total energy, considering the single.particle wavefunctions as parameters. Furthermore, the normalization constraint must be imposed via a Lagrange multiplier. Then, the question is to find \eqref{hamiltonian HF}'s minimum expectation value with respect to the ansatz given by \eqref{Hartree's ansatz}, with the normalization condition given by \eqref{HF normalization}, ie. 

\begin{align*}
    \bra{\Psi^{HF}}\hat{\bf H}\ket{\Psi^{HF}} &= \bra{ \prod_{j =1}^{n}\psi_{\alpha_j, s_j}(\x_j, \sigma_j)} \sum_i H^{(0)}(\x_i) + \sum_{i<j} v(r_{ij}) \ket{\prod_{j=1}^{n}\psi_{\alpha_j, s_j}(\x_j, \sigma_j) } \\
    &= \bra{\psi_{\bm{\alpha}_{1}}(\tilde{x}_1)} {H}^{(0)}(\x_1) \ket{\psi_{\bm{\alpha}_{1}}(\tilde{x}_1)} \cancelto{1}{\prod_{j=2}^{n} \bra{\psi_{\bm{\alpha}_{j}} (\tilde{x}_j)}\ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)}} \\
    %& + \bra{\psi_{\bm{\alpha}_{1}}(\tilde{x}_1)}\ket{\psi_{\bm{\alpha}_{1}}(\tilde{x}_1)} \bra{\psi_{\bm{\alpha}_{2}}(\tilde{x}_2)} {H}^{(0)}(\x_2) \ket{\psi_{\bm{\alpha}_{2}}(\tilde{x}_2)}\prod_{j=3}^{n} \bra{\psi_{\bm{\alpha}_{j}} (\tilde{x}_j)}\ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)} \\
    & + \cdots + \cancelto{1}{\prod_{j=1}^{L-1} \bra{\psi_{\bm{\alpha}_{j}} (\tilde{x}_j)}\ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)} }\bra{\psi_{\bm{\alpha}_{L}}(\tilde{x}_L)} {H}^{(L)}(\x_L) \ket{\psi_{\bm{\alpha}_{L}}(\tilde{x}_L)} \cancelto{1}{\prod_{k = L+1}^{n} \bra{\psi_{\bm{\alpha}_{k}} (\tilde{x}_j)}\ket{\psi_{\bm{\alpha}_{k}}(\tilde{x}_k)}} \\
    & + \cdots + \cancelto{1}{\prod_{j=1}^{n-1} \bra{\psi_{\bm{\alpha}_{j}} (\tilde{x}_j)}\ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)}} \bra{\psi_{\bm{\alpha}_{n}}(\tilde{x}_n)} {H}^{(n)}(\x_n) \ket{\psi_{\bm{\alpha}_{n}}(\tilde{x}_n)} \\
    &+ \bra{\psi_{\bm{\alpha}_{1}}(\tilde{x}_1) \psi_{\bm{\alpha}_{2}}(\tilde{x}_2)} v(r_{12}) \ket{\psi_{\bm{\alpha}_{1}}(\tilde{x}_1 \psi_{\bm{\alpha}_{2}}(\tilde{x}_2)} \cancelto{1}{\prod_{j=3}^{n} \bra{\psi_{\bm{\alpha}_{j}} (\tilde{x}_j)}\ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)}}  \\
    &+ \bra{\psi_{\bm{\alpha}_{1}}(\tilde{x}_1) \psi_{\bm{\alpha}_{3}}(\tilde{x}_3)} v(r_{13}) \ket{\psi_{\bm{\alpha}_{1}}(\tilde{x}_1) \psi_{\bm{\alpha}_{3}}(\tilde{x}_3)} \cancelto{1}{\bra{\psi_{\bm{\alpha}_{2}} (\tilde{x}_2)}\ket{\psi_{\bm{\alpha}_{2}} (\tilde{x}_2)}} \blanky \blanky \blanky \cancelto{1}{\prod_{j=4}^{n} \bra{\psi_{\bm{\alpha}_{j}} (\tilde{x}_j)}\ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)}} \\
    & + \cdots + \cancelto{1}{\prod_{j=1}^{L-2} \bra{\psi_{\bm{\alpha}_{j}} (\tilde{x}_j)}\ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)}} \bra{\psi_{\bm{\alpha}_{L-1}}(\tilde{x}_{L-1})\psi_{\bm{\alpha}_{L}}(\tilde{x}_{L})} v(r_{(L-1) \blanky L}) \ket{\psi_{\bm{\alpha}_{L-1}}(\tilde{x}_{L-1})\psi_{\bm{\alpha}_{L}}(\tilde{x}_{L})} \\
    & \times \cancelto{1}{\prod_{j=L}^{n} \bra{\psi_{\bm{\alpha}_{j}} (\tilde{x}_j)}\ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)}} + \cdots \\
    & \cdots + \cancelto{1}{\prod_{j=1}^{n-2} \bra{\psi_{\bm{\alpha}_{j}} (\tilde{x}_j)}\ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)}} \bra{\psi_{\bm{\alpha}_{n-1}}(\tilde{x}_{n-1})\psi_{\bm{\alpha}_{n}}(\tilde{x}_{n})} v(r_{(n-1) \blanky n}) \ket{\psi_{\bm{\alpha}_{L-1}}(\tilde{x}_{L-1})\psi_{\bm{\alpha}_{L}}(\tilde{x}_{L})} \\
\end{align*}

which then yields

\begin{align}
\blanky & \blanky & \blanky & \blanky & \blanky & \blanky & \blanky & \blanky & 
    \alignedbox{\bra{\Psi^{HF}}\hat{\bf H}\ket{\Psi^{HF}}=}{ \sum_{i=1}^{n} \bra{\psi_{\bm{\alpha}_{i}}(\tilde{x}_i)} {H}^{(0)}(\x_i) \ket{\psi_{\bm{\alpha}_{i}}(\tilde{x}_i)} + \sum_{i<j} \bra{\psi_{\bm{\alpha}_{i}}(\tilde{x}_{i})\psi_{\bm{\alpha}_{j}}(\tilde{x}_{j})} v(r_{ij})\ket{\psi_{\bm{\alpha}_{i}}(\tilde{x}_{i})\psi_{\bm{\alpha}_{j}}(\tilde{x}_{j})}}
    \label{pain}
\end{align}

In order to find \eqref{pain}'s minimum value, under certain conditions, the use of the Lagrange multipliers method is required. Consider a variation of \eqref{pain}, with respect to $\psi_{\bm{\alpha}_{j}}(\tilde{x}_{j})$, under the normalization condition \eqref{HF normalization} introduced via a Lagrange multiplier $\lambda$, given by

\begin{equation}
    \delta {\bra{\Psi^{HF}}\hat{\bf H}\ket{\Psi^{HF}}} - \sum_{i=1}^{n} \bigg[ \delta \bigg(\bra{\psi_{\bm{\alpha}_{i}}(\tilde{x}_{i})}\ket{\psi_{\bm{\alpha}_{i}}(\tilde{x}_{i}) - 1}\bigg) \bigg] = 0,
    \label{HF variation}
\end{equation}

for which, we first must calculate the variation.

\begin{align*}
    \delta {\bra{\Psi^{HF}}\hat{\bf H}\ket{\Psi^{HF}}} = \sum_{i=1}^{n} & \bra{ \delta \psi_{\bm{\alpha}_{i}}(\tilde{x}_i)} {H}^{(0)}(\x_i) \ket{\psi_{\bm{\alpha}_{i}}(\tilde{x}_i)} + \bra{  \psi_{\bm{\alpha}_{i}}(\tilde{x}_i)} {H}^{(0)}(\x_i) \ket{\delta \psi_{\bm{\alpha}_{i}}(\tilde{x}_i)} \\
    &+ \sum_{i<j} \bra{\delta \psi_{\bm{\alpha}_{i}}(\tilde{x}_{i})\psi_{\bm{\alpha}_{j}}(\tilde{x}_{j})} v(r_{ij})\ket{\psi_{\bm{\alpha}_{i}}(\tilde{x}_{i})\psi_{\bm{\alpha}_{j}}(\tilde{x}_{j})} \\
    &+ \sum_{i<j} \bra{ \psi_{\bm{\alpha}_{i}}(\tilde{x}_{i})\delta \psi_{\bm{\alpha}_{j}}(\tilde{x}_{j})} v(r_{ij})\ket{\psi_{\bm{\alpha}_{i}}(\tilde{x}_{i})\psi_{\bm{\alpha}_{j}}(\tilde{x}_{j})} \\
    &+ \sum_{i<j} \bra{ \psi_{\bm{\alpha}_{i}}(\tilde{x}_{i}) \psi_{\bm{\alpha}_{j}}(\tilde{x}_{j})} v(r_{ij})\ket{\delta \psi_{\bm{\alpha}_{i}}(\tilde{x}_{i})\psi_{\bm{\alpha}_{j}}(\tilde{x}_{j})} \\
    &+\sum_{i<j} \bra{ \psi_{\bm{\alpha}_{i}}(\tilde{x}_{i}) \psi_{\bm{\alpha}_{j}}(\tilde{x}_{j})} v(r_{ij})\ket{ \psi_{\bm{\alpha}_{i}}(\tilde{x}_{i}) \delta\psi_{\bm{\alpha}_{j}}(\tilde{x}_{j})} \\
    &= \sum_{i=1}^{n} \bigg[ \bra{\delta \psi_{\bm{\alpha}_{i}}(\tilde{x}_{i})} \bigg( H^{(0)}(\x_i) + \sum_{\substack{j = 1 \\
    j \neq i}}^{n} \bra{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)}
    v(r_{ij})
    \ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)} \bigg) \ket{\psi_{\bm{\alpha}_{i}}(\tilde{x}_{i})} \bigg] \\
    &  + \sum_{i=1}^{n} \bigg[ \bra{ \psi_{\bm{\alpha}_{i}}(\tilde{x}_{i})} \bigg( H^{(0)}(\x_i) + \sum_{\substack{j = 1 \\
    j \neq i}}^{n} \bra{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)}
    v(r_{ij})
    \ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)} \bigg) \ket{ \delta \psi_{\bm{\alpha}_{i}}(\tilde{x}_{i})} \bigg].
\end{align*}

Note that the sum over $i < j$ is now a sum over $i \neq j$. Combining the previous results with \eqref{HF variation} yields

\begin{equation}
    \begin{array}{c}
        \sum_{i=1}^{n} \bigg[ \bra{\delta \psi_{\bm{\alpha}_{i}}(\tilde{x}_{i})} \bigg( H^{(0)}(\x_i) + \sum_{\substack{j = 1 \\
    j \neq i}}^{n} \bra{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)}
    v(r_{ij})
    \ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)} - \lambda_i \bigg) \ket{\psi_{\bm{\alpha}_{i}}(\tilde{x}_{i})}  \bigg]  \\
    \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky \blanky + \sum_{i=1}^{n} \bigg[ \bra{ \psi_{\bm{\alpha}_{i}}(\tilde{x}_{i})} \bigg( H^{(0)}(\x_i) + \sum_{\substack{j = 1 \\
    j \neq i}}^{n} \bra{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)}
    v(r_{ij}) 
    \ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)} - \lambda_i \bigg) \ket{ \delta \psi_{\bm{\alpha}_{i}}(\tilde{x}_{i})} \bigg] = 0
    \end{array},
\end{equation}

which must be true for all variations. Therefore, 

\begin{equation}
    \bigg[H^{(0)}(\x_i) +  \sum_{\substack{j = 1 \\
    j \neq i}}^{n} \bra{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)}
    v(r_{ij}) 
    \ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)} \bigg] \ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)} = \lambda_i \ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)},
\end{equation}

and considering that $\x_j$ is an auxiliary variable, an integrand variable, this can be rewritten as 

\begin{equation*}
    \bigg[H^{(0)}(\x_i) +  \sum_{\substack{j = 1 \\
    j \neq i}}^{n} \int_{\R^3} d\x \frac{n_{\alpha_j}(\x)}{|\x_i - \x|} \bigg] \ket{\psi_{\bm{\alpha}}(\x)} = \lambda_i \ket{\psi_{\bm{\alpha}_{j}}(\tilde{x}_j)},
\end{equation*}

or more generally, 

\begin{align} 
    & \bigblanky & \bigblanky & \alignedbox{\bigg[-\frac{1}{2} \nabla^2 + {\bf V}_{\textnormal{ext}}(\x_i) + \sum_{\bm{\lambda} \in \Lambda, \blanky \bm{\lambda} \neq \bm{\alpha}} \int_{\R^3} d\x \frac{n_{\bm{\lambda}}(\x)}{|\x_i - \x|} \bigg] \psi_{\bm{\alpha}}(\x)}{ = \lambda_{\bm{\alpha}}  \psi_{\bm{\alpha}}(\x)},
\end{align} 

ie. the Hartree-Fock equations, written in terms of a set of $n$ Lagrange multipliers, $\{\lambda_i\}_{i=1}^{n}$. These Lagrange multipliers, though having dimensions of energy, a priori, bear no connection with the electron's energies. It turns out they have a precise physical meaning, which is established by Koopman's theorem.

\clearpage

\section{Observables}

The general purpose for solving the eigenvalue equation \eqref{ev equation} is to then use said wave-function to calculate observables. Consider the grand canonical ensemble and a given operator $\hat{\bf O} : \mathds{H} \rightarrow \mathds{H}$, then 

\begin{equation}
\langle \hat{\bf O} \rangle = \frac{1}{\mathcal{Z}} \sum_{\bm{\alpha }\in \Lambda} e^{-\beta(E_{\bm{\alpha} }- \mu N_{\bm{\alpha})}} \bra{\Psi_{\bm{\alpha}}}\hat{\bf O}\ket{\Psi_{\bm{\alpha}}},
\label{grand canonical}
\end{equation}

where 

\begin{itemize}
    \item $\bm \alpha \in {\Lambda}$ indexes the different states, 
    \item where $\beta = \frac{1}{kT}$ and $\mu$ is the chemical potential,
    \item $E_{\bm{\alpha}}$ and $N_{\bm{\alpha}}$ are the energy and particle number in state $\ket{\Psi_\alpha}$ respectively,
    \item and where $\mathcal{Z} =  \sum_{\alpha \in \Lambda} e^{-\beta(E_{\bm \alpha} - \mu N_{\bm \alpha})}$ is the partition function. 
\end{itemize}

In other words, \eqref{grand canonical}, the operator $\hat{\bf O}$ stands for the measurement and is integrated with all the many-body states $\ket{\Psi_\alpha} \in \mathds{H} \approx L^2(\R^{3N}) \otimes (\mathds{C}^{2})^{\otimes N}$ which describe the system. \\

At $N$ fixed and a zero temperature, \eqref{grand canonical} means:

\begin{equation}
     \langle \hat{\bf O} \rangle = \int_{\mathds{R}^{3N}} \prod_{i=1}^N d{\bf x}_i \prod_{j=1}^{N} \sum_{\sigma_j \in \mathds{C}^2} \Psi^{*}(\tilde{x}_1, \cdots, \tilde{x}_N) \sum_{a,b,\cdots} O(x_a, x_b, \cdots) \Psi(\tilde{x}_1, \cdots, \tilde{x}_N),
\label{ensemble avg}
\end{equation}

which can be thought as a functional of the many-body wave function, $\langle \hat{\bf O} \rangle = \langle \hat{\bf O} \rangle[\Psi]: {V}^{\mathds{H}} \rightarrow \R$, with $V = \R^{3N} \times \mathds {C}^{2N}$. Now, note that many details of the many-body wavefunction are integrated out. \\

One way to resolve the many-body problem consists in using an approximate wave-function. For example, Hartree-Fock, configuration interaction and Quantum Monte-Carlo are based in this approach. Another, more radical approach, consists in asking ourselves if we really need to work with all possible many-body wave-functions. \\

Note that \eqref{ensemble avg} depends, at most, with the form of the Hamiltonian, which in our case is given by 
 
\begin{equation}
    \hat{\bf H} = -\frac{\hbar^2}{2m_e} \sum_i \nabla^2_i + \sum_{i} {\bf V}_{\textnormal{ext}} ({\bf x}_i) + \frac{1}{2} \sum_{i \neq j} \frac{e^2}{|{\bf x}_i-{\bf x}_j|}.
\label{hamiltonian 3}
\end{equation}

We consider that the kinetic energy operator and the Coulomb interaction contain enough information to completely determine the value of an observable. Therefore, we can consider the expectation value to be a functional on the external potential, instead of a functional of the many-body wave-function:

$$
\langle \hat{\bf O} \rangle = \langle \hat{\bf O}  \rangle[\Psi] \Rightarrow
\langle \hat{\bf O}  \rangle[V_{\textnormal{ext}}]
: {V}^{\mathds{H}} \rightarrow \R.
$$

Thinking this in terms of functionals, eliminate the explicit dependence on the coordinates $(\tilde{x}_1, \cdots, \tilde{x}_n)$. In order to understand this new functional we note, that in the grand canonical ensemble, the sum of states is actually a trace over the Hilbert space's states, and since the trace is invariant, we can re-write \eqref{ensemble avg} as 

\begin{equation}
    \langle \hat{\bf O} \rangle = \frac{1}{\mathcal{Z}} \Tr_{\mathds{H}} \bigg(e^{-\beta(\hat{\bf H}-\mu\hat{\bf N})} \hat{\bf O} \bigg),
\label{trace gce}
\end{equation}

with the Hamiltonian given by \eqref{hamiltonian 3}. Therefore \eqref{trace gce} depends only on the external potential. Thus, we have expressed a rather complicated functional in terms of a simple function, which we can then try to approximate. \\

\section{Subsets of interesting observables}

Consider an electronic structure, what are the typical observables of interest? \\

A first example could be the equilibrium distance between two atoms in a molecule, which is the result of an energy-minimization problem. The energy minimization can also give the full geometry of a molecule. 
For example, 

\begin{itemize}
    \item consider a carbon dioxide molecule, the energy minimization problem predicts a linear structure with a flat angle between the two oxygens. Likewise, for the water molecule, the energy minimization problem predicts a specific angle between the hydrogens in said molecule. The same can be said about the ammonia, which is not a planar molecule. 
    \item The crystal structure can also be predicted in this way, by evaluating the forces acting on each atom of the system.
    \item In particular, this method can also compute the lattice vibrations that give rise to phonons or the bulk modulus, the ability of a certain material to resist to a certain external pressure. \\
\end{itemize}

These are very specific examples in which the core of problem is to evaluate the ground-state energy. In DFT, even the excited states of a system are observables as well as some response functions, such as optical absorption or electron energy loss. \\

\section{Observables in terms of Compact Quantities}

This section's aim is to find a simple quantity, in terms of which we can express the observables. Let $(\mathcal{B}(\mathds{H}), ||\cdot||)$ denote the space of all linear operators acting on the Hilbert space, noting that the sub-space of all linear bounded operators is a Banach space. Let $\mathcal{B}_1(\mathds{H})=\{\hat{\bf O}| \hat{\bf O}:\mathds{H}_k \rightarrow \mathds{H}_k \}$ ie. the space of all one-body operators, then we define the space of $k$-body operators as 

$$
\mathcal{B}_k(\mathds{H}) = \{\otimes_{i=1}^{k} {\bf O}_i | {\bf O}_i \in \mathcal{B}_1(\mathds{H}) \},
$$

where, obviously,

$$
\mathcal{B}(\mathds{H}) = \bigsqcup_{i=1}^{N} \mathcal{B}_i(\mathds{H}).
$$

Now, consider a one-body operator $\hat{\bf Q}_i \in \mathcal{B}_{1}(\mathds{H})$, which acts on the $i$-th electron's Hilbert space $\mathds{H}_i$, which we desire to calculate its expected value following equation \eqref{ensemble avg}. Therefore, we can construct a new operator 
$$
\hat{\bf Q} = \bigoplus_{i=1}^{N} \hat{\bf Q}_i = \sum_{i=1}^{N} \tilde{\bf Q}_i
$$

where now both $\hat{\bf Q}$ and $\tilde{\bf Q}_i$ act on the full Hilbert space. Note that $\tilde{\bf Q}_i$ acts trivially on all but one of the individual Hilbert spaces:

\begin{align}
     \langle \hat{\bf Q} \rangle &= \int_{\mathds{R}^{3N}} \prod_{i=1}^N d{\bf x}_i \prod_{j=1}^{N} \sum_{\sigma_j \in \mathds{C}^2} \Psi^{*}(\tilde{x}_1, \cdots, \tilde{x}_N)
     \sum_{i}
     \tilde{\bf Q}_i(\tilde{x_i})
     \Psi(\tilde{x}_1, \cdots, \tilde{x}_N) \\
     &= N \int_{\mathds{R}^{3N}} \prod_{i=1}^N d{\bf x}_i \sum_{\sigma_1 \in \mathds{C}^2} \Psi^{*}(\tilde{x}_1, \cdots, \tilde{x}_N)
     \tilde{\bf Q}_1(\tilde{x}_1)
     \Psi(\tilde{x}_1, \cdots, \tilde{x}_N) \\
     &= \int_{\mathds{R}^{3}} d{\bf x}_1 \sum_{\sigma_1 \in \mathds{C}^2} \tilde{\bf Q}_1(\tilde{x}_1) N \int_{\mathds{R}^{3(N-1)}} \prod_{i=2}^N d{\bf x}_i \prod_{j=2}^{N} \sum_{\sigma_j \in \mathds{C}^2} \Psi^{*}(\tilde{x}_1, \cdots, \tilde{x}_N)
     \Psi(\tilde{x}_1, \cdots, \tilde{x}_N) \\
     &= \int_{\mathds{R}^{3}} d{\bf x}_1 \sum_{\sigma_1 \in \mathds{C}^2} \tilde{\bf Q}_1(\tilde{x}_1) {\bf n}(\tilde{x}_1),
\label{one-body ensemble avg}
\end{align}

where 
 
\begin{equation}
{\bf n}(\tilde{x}_1) = N \int_{\mathds{R}^{3(N-1)}} \prod_{i=2}^N d{\bf x}_i \prod_{j=2}^{N} \sum_{\sigma_j \in \mathds{C}^2} \Psi^{*}(\tilde{x}_1, \cdots, \tilde{x}_N)
     \Psi(\tilde{x}_1, \cdots, \tilde{x}_N),
\label{one-body density}
\end{equation}

is the one-body density. Thus, in order to calculate the expectation value of a one-body operator, we need only the one-body density. Similarly, if we want to compute the expectation value of a general two-body operator, the same steps can be repeated. Consider, for example, the Coulomb interaction. Then it's expectation value is 

\begin{align}
     V_{ee} &= \int_{\mathds{R}^{3N}} \prod_{i=1}^N d{\bf x}_i \prod_{j=1}^{N} \sum_{\sigma_j \in \mathds{C}^2} \Psi^{*}(\tilde{x}_1, \cdots, \tilde{x}_N)
     \sum_{\substack{i,j\\
          j > i}} 
     \frac{e^2}{|{\bf x}_i-{\bf x}_j|}
     \Psi(\tilde{x}_1, \cdots, \tilde{x}_N) \\
     &= \int_{\mathds{R}^{6}} d{\bf x}_1 d{\bf x}_2 \sum_{\sigma_1 \in \mathds{C}^2} \sum_{\sigma_2 \in \mathds{C}^2} \frac{1}{2} \frac{e^2}{|{\bf x}_1-{\bf x}_2|} N(N-1) \int_{\mathds{R}^{3(N-2)}} \prod_{i=3}^N d{\bf x}_i \prod_{j=3}^{N} \sum_{\sigma_j \in \mathds{C}^2} \Psi^{*}(\tilde{x}_1, \cdots, \tilde{x}_N)
     \Psi(\tilde{x}_1, \cdots, \tilde{x}_N) \\
     &=\int_{\mathds{R}^{6}} d{\bf x}_1 d{\bf x}_2 \sum_{\sigma_1 \in \mathds{C}^2} \sum_{\sigma_2 \in \mathds{C}^2} \frac{1}{2} \frac{e^2}{|{\bf x}_1-{\bf x}_2|} {\bf n}^{(2)}(\tilde{x}_1, \tilde{x}_2),
\label{two-body ensemble avg}
\end{align}

where

$$
{\bf n}^{(2)}(\tilde{x}_1, \tilde{x}_2) = N(N-1) \int_{\mathds{R}^{3(N-2)}} \prod_{i=3}^N d{\bf x}_i \prod_{j=3}^{N} \sum_{\sigma_j \in \mathds{C}^2} \Psi^{*}(\tilde{x}_1, \cdots, \tilde{x}_N)
     \Psi(\tilde{x}_1, \cdots, \tilde{x}_N),
$$

is the two-body density.  \\

One interesting observable may be the kinetic  energy, which can readily be written as 

\begin{align}
     T &= -\int_{\mathds{R}^{3N}} \prod_{i=1}^N d{\bf x}_i \prod_{j=1}^{N} \sum_{\sigma_j \in \mathds{C}^2} \Psi^{*}(\tilde{x}_1, \cdots, \tilde{x}_N)
     \sum_{i}\frac{\nabla^{2}_{i}}{2}
     \Psi(\tilde{x}_1, \cdots, \tilde{x}_N) \\
     &= - \int_{\mathds{R}^{3}} d{\bf x}_1 \sum_{\sigma_1 \in \mathds{C}^2} \bigg[\frac{\nabla^{'2}_1}{2} N \int_{\mathds{R}^{3(N-1)}} \prod_{i=2}^N d{\bf x}_i \prod_{j=2}^{N} \sum_{\sigma_j \in \mathds{C}^2} \Psi^{*}(\tilde{x}_1, \cdots, \tilde{x}_N) \Psi(\tilde{x}'_1, \cdots, \tilde{x}_N)\bigg]_{{x'_1 \rightarrow x_1}} \\
     &= \int_{\mathds{R}^{3}} d{\bf x}_1 \sum_{\sigma_1 \in \mathds{C}^2} \frac{\nabla^{'2}_1}{2} {\rho}(\tilde{x}_1, \tilde{x}'_1)_{{x'_1 \rightarrow x_1}},
\end{align}

where, once again, $\rho$ is the one-density matrix 

$$
{\rho}(\tilde{x}_1, \tilde{x}'_1) =  N \int_{\mathds{R}^{3(N-1)}} \prod_{i=2}^N d{\bf x}_i \prod_{j=2}^{N} \sum_{\sigma_j \in \mathds{C}^2} \Psi^{*}(\tilde{x}_1, \cdots, \tilde{x}_N) \Psi(\tilde{x}'_1, \cdots, \tilde{x}_N).
$$

For example, in outlook spectroscopy, other useful ingredients may be the observables associated with transitions to excited states, ie.

\begin{align}
f_{s}^{eh}(\tilde{x}_1, t) &= \int_{\mathds{R}^{3(N-1)}} \prod_{i=2}^N d{\bf x}_i \prod_{j=2}^{N} \sum_{\sigma_j \in \mathds{C}^2} \Psi^{*}(\tilde{x}_1, \cdots, \tilde{x}_N; t) \Psi_{s}(\tilde{x}_1, \cdots, \tilde{x}_N; t) \\
&= e^{-i(E_s-E_0)t} \int_{\mathds{R}^{3(N-1)}} \prod_{i=2}^N d{\bf x}_i \prod_{j=2}^{N} \sum_{\sigma_j \in \mathds{C}^2} \Psi^{*}(\tilde{x}_1, \cdots, \tilde{x}_N) \Psi_{s}(\tilde{x}_1, \cdots, \tilde{x}_N) 
\end{align}

said transition depending on a given number $s$, which indexes the excited state to which the electron has transitioned and where a phase factor has appeared on the second line, an oscillation on the excitation energy. \\

Up to now, in order to calculate the expected value of different $k$-body operators, we need different $N-k$-body kernels, to which integrate them with. Therefore, we first need to calculate said $N-k$-body kernels. However, we insist on finding only one kernel for all different $K-$body operators. \\

%Consider first one-body operators, then the expected values are only functionals of the one-body density given by \eqref{one-body density}.

\section{The Hohenberg-Kohn Theorem}

\subsection{Calculus of Variations}

A functional is a mapping $F[\cdot]: C^{\infty}(\mathds{R})\rightarrow\mathds{R}$. Integrals, for example, can be thought as functionals, values of a function at a certain point $x_0 \in \mathds{R}$ can also be thought as a functional, with a weight function $W(x) = \delta(x-x_0)$. Some other interesting functionals are, for example, 

\begin{itemize}
    \item Local functional: $F[f] = \int_{\mathds{R}} dx \blanky g(f(x))$,
    \item Non-local functionals: 
    $F[f_1, f_2] =\int_{\mathds{R}^2} dx dx' \blanky W(x,x') f_1(x) f_2(x')$, 
    \item the total energy of a many-body system 
    $$
    E = F[\Psi] = \int_{\mathds{R}^{3N}} \prod_{i=1}^{N} d{\bf x}_i \blanky \Psi^{*}({\bf x}_1, \cdots, {\bf x}_N) {\bf H} \Psi({\bf x}_1, \cdots, {\bf x}_N) = \bra{\Psi}{\bf H}\ket{\Psi}.
    $$
    \item The Lagrangian action: $
    S[{\bf q}] = \int dt \blanky \mathcal{L}({\bf q}(t), \dot{\bf q}(t), t).$
\end{itemize}

In the context of the many-body problem, some interesting functionals of the electron density, could be 

\begin{itemize}
    \item The contribution of the external energy given by an external potential
    
    $$
    E_{\textnormal{ext}}[n] = \int_{\mathds{R}^3} d{\bf x}  \blanky n({\bf x}) V_{\textnormal{ext}}({\bf x}),
    $$
    
    which is a local functional.
    \item The classical electrostatic term, also called the Hartree energy, is a non-local functional:
    
    $$
    E_{\textnormal{H}}[n] = \frac{1}{2} \int_{\mathds{R}^6} d{\bf x} d{\bf x}' \blanky \frac{n({\bf x})n({\bf x'})}{|{\bf x}-{\bf x}'|}.
    $$
    
    \item Another interesting functional is the Thomas-Fermi approximation, in which the kinetic energy is a $5/3$-power of the electron density
    
    $$
    T^{TF}[n] = C \int_{\mathds{R}^3} d{\bf x} \blanky n^{\frac{5}{3}}({\bf x}),
    $$
    
    \item or the von Weizs\"acker functional:
    
    $$
    T^{vW}[n] = \frac{1}{8} \int_{\mathds{R}^3} d{\bf x} \blanky \frac{\nabla n({\bf x}) \cdot \nabla n({\bf x})}{n({\bf x})}. 
    $$
\end{itemize}

\blanky \\

\subsection{Functional Derivatives}

In order to construct a functional derivative, we need to consider a generic function $f \in C^{\infty}(\R)$ and a perturbation $\eta(x)$:

\begin{align*}
    F[f(x) + \epsilon \eta(x)] = F[f] + \frac{dF}{d\epsilon}[f+\epsilon \eta]\bigg|_{\epsilon = 0} \epsilon + \frac{1}{2} \frac{d^2 F}{d\epsilon^2}[f+\epsilon \eta]\bigg|_{\epsilon = 0} \epsilon^2 + \mathcal{O}(\epsilon^3).
\end{align*}

Therefore, we can define 

$$
\frac{dF}{d\epsilon}[f+\epsilon \eta]\bigg|_{\epsilon = 0} = \frac{ F[f(x) + \epsilon \eta(x)]  - F[f]}{\epsilon}.
$$

We then define the first and second order functional derivatives as 

\begin{align}
\frac{dF}{d\epsilon}[f+\epsilon \eta]\bigg|_{\epsilon = 0} &= \int_{\R} dx \blanky \frac{\delta F[f]}{\delta f(x)} \eta(x),
& \frac{d^2 F}{d\epsilon^2}[f+\epsilon \eta]\bigg|_{\epsilon = 0} &= \int_{\R^2} dx dx' \blanky \frac{\delta^2 F[f]}{\delta f(x) \delta f(x')} \eta(x) \eta(x').
\end{align}

and so on. Consider now, for example, a local functional of the form

$$
F[f] = \int_{\R_{[a,b]}} dx \blanky g(x, f(x), f'(x)),
$$

which we'd like to calculate its functional derivative. In order to do exactly that, we Taylor-expand the functional in terms of the incremented function 

\begin{align*}
    F[f+\epsilon \eta] = F[f] + \frac{d}{d\epsilon} \int_{\R_{[a,b]}} dx \blanky g(x, f+\epsilon \eta, f'+\epsilon \eta')\bigg|_{\epsilon = 0} \epsilon, \\
    \Rightarrow \frac{d}{d\epsilon} \int_{\R_{[a,b]}} dx \blanky g(x, f+\epsilon \eta, f'+\epsilon \eta')\bigg|_{\epsilon = 0} \epsilon &= \frac{d}{d\epsilon} \int_{\R_{[a,b]}} dx \blanky\bigg[ g(x,f,f') + \epsilon \eta \frac{\partial g}{\partial f}  + \epsilon \eta' \frac{\partial g}{\partial f'} \bigg]\bigg|_{\epsilon = 0} \epsilon \\
    &= \int_{\R_{[a,b]}} dx \blanky\bigg[\eta \frac{\partial g}{\partial f}  + \eta' \frac{\partial g}{\partial f'} \bigg] \epsilon \\
    &= \cancel{\eta \frac{\partial g}{\partial f'}\bigg|_{a}^{b}} \epsilon + \int_{\R_{[a,b]}} dx \blanky\bigg[ \frac{\partial g}{\partial f}  - \frac{d}{dx}\frac{\partial g}{\partial f'} \bigg] \eta (x) \epsilon \\
    &= \int_{\R} dx \blanky \frac{\delta F[f]}{\delta f(x)} \eta(x),\\
\Rightarrow \frac{\delta F[f]}{\delta f(x)} = \frac{\partial g}{\partial f}  - \frac{d}{dx}\frac{\partial g}{\partial f'}, \textnormal{ the Euler-Lagrange equations.} 
 \end{align*}
 
\clearpage

\subsection{Defining the system via an external potential}

Using the previous section's techniques, we'd like to inquire if we can consider a functional of the density as the unique functional of a quantum mechanical observation. Indeed, it's possible to demonstrate that every observable in the ground state is a unique functional of the electron density. This is the main outcome of the Hohenberg-Kohn theorem. \\

We start by saying that every observable is a functional on the many-body wave-function: $\mathcal{O} = \bra{\Psi}\hat{\bf O}\ket{\Psi} = \mathcal{O}[\Psi]$. Now, consider a non-relativistic many-body Hamiltonian in the Born-Oppenheimer, which is given by \eqref{hamiltonian 3}. Note that the most important term in said expression, the operator which distinguishes a many-body problem from another, is the external potential\footnote{For example an atom of nickel contains 28 electrons, the same as the bulk silicon. The structural differences of said many-body systems lie on the different strength and position of the external (nuclei) potentials.}. Since the external potential defines the many-body system, once it's specified, it gives a unique Hamiltonian, which can then be diagonalized to find its eigenvectors and eigenfunctions. \\

Then, if the expectation value of an operator depends on the wave-function, eg. the ground state function, which in turns depends on the Hamiltonian and hence on the external potential, can we say that each observable is uniquely determined by the external potential? In other words, can we say that each observable is a unique functional of the external potential? \\

If that's the case, then there has to be an isomorphism from the space of all external potentials (which can be thought as $C^\infty(\R)$) to the space of all distinct many-body system's wave-functions. This is there has to be a bijection, two different potentials lead to two different wave-functions and vice-versa. Is this the case? At the moment, we cannot say, but if, from a given potential, we obtain a single ground state wave function, thus this mapping is surjection and is then a function. This isn't the case for many-body systems with a degenerate ground state. Then, we can modify our statement as follows: \textit{there is an isomorphism from the space of all external potentials to the space of all wave-functions abiding the same ground state energy}. For all practical purposes, there is a function, an surjective mapping, from the space of all external potentials to the space of all distinct many-body system's wave-functions. \\

It begs the question if this mapping is an injective mapping, should this be the case, there'd be a one-to-one correspondence between the external potential and the wave-function. It would then mean that any observable is a unique functional of the external potential. Another interesting question is if the same argument is valid, not for the wave-functions, but with the one-body densities. The answer to these questions constitute the Hohenberg-Kohn theorem. \\

\subsection{Proof of the Hohenberg-Kohn Theorem}

There are actually two theorems attributed to Hohenberg and Kohn, which relate to any system consisting of electrons moving under the influence of an external potential $v_{\textnormal{ext}}({\bf x})$. In the following discussion, we'll consider only the cases where the mapping, from the space of external potentials to the space of all distinct $N$-electrons many-body groundstate wave-functions, is a function, ie. we exclude the cases of obtaining a degenerate groundstate wave-function. \\

\begin{theo}
The external potential $v_{\textnormal{ext}}({\bf x})$, and hence the total energy, is a unique functional of the electron density $n({\bf x})$.
\end{theo}

\begin{proof}
The energy functional $E[n({\bf x})]$ alluded to in the first Hohenberg-Kohn theorem can be written in terms of the external potential $v_{\textnormal{ext}}({\bf x})$ in the following way 
\begin{equation}
    E_{\textnormal{ext}}[n] = \int_{\mathds{R}^3} d{\bf x}  \blanky n({\bf x}) V_{\textnormal{ext}}({\bf x}) + F[n({\bf x})],
\end{equation}

where $F[n({\bf x})]$ is an unknown, but otherwise universal functional of the electron density $n({\bf x})$ only. Correspondingly, a Hamiltonian for the system can be written in such a way that the electron wave-function, $\Psi$, which minimises the expectation value gives the ground state energy, assuming a non-degenerate groundstate: $E_{\textnormal{ext}}[n] = \bra{\Psi}\hat{\bf H}\ket{\Psi}$. 
The Hamiltonian can then be written as 

$$
\hat{\bf H} = \hat{\bf F} + \hat{V}_{\textnormal{ext}},
$$

where $\hat{\bf F}$ is the electronic Hamiltonian consisting of a kinetic energy operator and an interaction operator. The electron operator $\hat{\bf F}$ is the same for all $N$-electron systems. Therefore, the Hamiltonian ${\bf H}$ is completely defined by the number of electrons $N$ and the external potential $\hat{V}_{\textnormal{ext}}$. The proof of the first theorem lies on considering two different external potentials, say $\hat{V}^{(1)}_{\textnormal{ext}}$ and $\hat{V}^{(2)}_{\textnormal{ext}}$, which we assume to give rise to the same density $n_{0}({\bf x})$. The associated Hamiltonians, labelled $\hat{\bf H}^{(1)}$ and $\hat{\bf H}^{(2)}$, will therefore have different groundstate wave-functions ${\Psi}^{(1)}$ and ${\Psi}^{(2)}$, which yield the same density $n_{0}({\bf x})$. Using the variational principle, we have 

\begin{align}
    E_{0}^{(1)} &< \bra{\Psi_2} \hat{\bf H}_1 \ket{\Psi_2} = \bra{\Psi_2} \hat{\bf H}_2 \ket{\Psi_2} + \bra{\Psi_2} \hat{\bf H}_1-\hat{\bf H}_2 \ket{\Psi_2} \\
    &=  E_{0}^{(2)} + \int_{\R^3} d{\bf x} \blanky n_{0}({\bf x}) [\hat{V}^{(1)}_{\textnormal{ext}}({\bf x})-\hat{V}^{(2)}_{\textnormal{ext}}({\bf x})]
\end{align}

where $E_{0}^{(1)}$ and $E_{0}^{(2)}$ are the groundstates energies of $\hat{\bf H}^{(1)}$ and $\hat{\bf H}^{(2)}$ respectively. It's at this point that the Hohenberg-Kohn theorems, and therefore DFT, apply rigorously to the groundstate only. Note that the previous expression holds as well when the subscripts are interchanged. Thus leading to $ E_{0}^{(1)} +  E_{0}^{(2)} <  E_{0}^{(2)} +  E_{0}^{(1)}$, which is a contradiction, and as a result the groundstate density uniquely determines the external potential (upto an additive constant). \\
\end{proof}

Stated simply, the electrons determine the positions of the nuclei in a system, and also all groundstate electronic properties. Therefore, the external potential and $N$ completely determine the Hamiltonian. \\

The second Hohenberg-Kohn theorem states 

\begin{theo}
\end{theo}
The groundstate energy can be obtained variationally: the density that minimises the total energy is the exact groundstate density. 

\begin{proof}

According to the first Hohenberg-Kohn theorem, $n({\bf x})$ determines $\hat{V}_{\textnormal{ext}}$. $N$ and $\hat{V}_{\textnormal{ext}}$ completely determine $\hat{\bf H}$ and therefore $\Psi$. Therefore, $\Psi$ is a functional of the density, which implies that the expectation value of the electronic Hamiltonian, $\hat{\bf F}$, is a functional of the density as well: $\hat{\bf F} [n({\bf x})] = \bra{\Psi}\hat{\bf F}\ket{\Psi}$. \\

A density that is the groundstate of some external potential is known as $v$-representable. Then, a $v-$representable energy functional $E_{v}[n({\bf x})]$ can be defined in which the external potential $v({\bf x})$ is unrelated to another density $n'({\bf x})$,


\begin{align*}
    E_{v}[n'({\bf x})] = \int_{\R^3} d{\bf x} \blanky n'({\bf x}) v({\bf x}) + \hat{\bf F} [n'({\bf x})],
\end{align*}

then the variational principle asserts,

\begin{equation*}
    \bra{\psi'}\hat{\bf F}\ket{\psi'} + \bra{\psi'}\hat{V}_{\textnormal{ext}}\ket{\psi'} >  \bra{\psi}\hat{\bf F}\ket{\psi} + \bra{\psi}\hat{V}_{\textnormal{ext}}\ket{\psi},
\end{equation*}

where $\psi$ is the wave-function associated with the correct groundstate $n({\bf x})$. This leads to, 

\begin{align*}
     \int_{\R^3} d{\bf x} \blanky n'({\bf x})\hat{V}_{\textnormal{ext}} + \hat{\bf F} [n'({\bf x})] > \int_{\R^3} d{\bf x} \blanky n({\bf x})\hat{V}_{\textnormal{ext}} + \hat{\bf F} [n({\bf x})] \\
     \Rightarrow  E_{v}[n'({\bf x})] >  E_{v}[n({\bf x})],
\end{align*}

which proves the second Hohenberg-Kohn theorem. 

\end{proof}

These two theorems are extremely powerful, though by themselves, do not offer a way of computing the groundstate density in practice. \\

\subsection{Insights into the Hohenberg-Kohn theorems}

Every observable in the groundstate is a unique functional of the electron density. In this section, we'll explore some of the immediate consequences of the previous theorems. \\

Consider the total energy, a measurable quantity with the associated observable being the Hamiltonian ie. 

\begin{equation}
    E[n] = \bra{\Psi} \hat{\bf T}+\hat{\bf V}_{ee}+\hat{V}_{\textnormal{ext}}\ket{\Psi} \rightarrow  E[n] = \bra{\Psi} \hat{\bf H}\ket{\Psi}.
    \label{total energy functional}
\end{equation}

In particular, for the groundstate state total energy, the variational theorem holds, stating that 

\begin{equation}
    \min_{\Psi \in \mathds{H}} \bra{\Psi} \hat{\bf H}\ket{\Psi} =  \bra{\Psi_0} \hat{\bf H}\ket{\Psi_0} = E_0.
\end{equation}

And, given that there is a one-to-one correspondence between a wave-function and a density, the groundstate density corresponds to the groundstate wave-function. Therefore, this minimization can be done directly on the density itself 

\begin{equation}
\min_{n({\bf x})} E[n] = E_0,
\label{variational dft}    
\end{equation}

which is a direct consequence of the second Hohenberg-Kohn theorem. \eqref{variational dft} shows a practical way, via a variational procedure, to find an observable as a density functional. Indeed, rather than using the variational theorem in the realm of all the possible wave functions that live in a $3N$-dimensional space for which the energy surface can be enormously complicated, we can restrict our search to the realm of the densities that live in a smaller-dimensional space, thanks to the Hohenberg-Kohn theorems. \\

For a given density profile, call it $n_1$, there's a unique certain total energy $E[n_1]$. For another density, $n_2$, there's a different total energy $E[n_2]$, which is also unique. This is true in general: $\{n_\lambda\}_{\lambda \in \Lambda} \rightarrow \{E[n_\lambda]\}_{\lambda \in \Lambda}$. Therefore, we can find the minimum of the total energy, which constitutes the groundstate total energy, in correspondence with the groundstate density $n_0$. In practice, this is not the way in which most DFT calculations are performed, but it gives, in principle, a methodology for numerically calculating it. However, there's an unknown relationship between the density and the total energy since we don't know the functional. Formally, the total energy can be written as \eqref{total energy functional}, which can be re-written as 

\begin{align}
    E[n] = \bra{\Psi} \hat{\bf T}+\hat{\bf V}_{ee}\ket{\Psi} + \int_{\R^3} d{\bf x} \blanky \hat{V}_{\textnormal{ext}}({\bf x}) n({\bf x}) \\
    = \hat{\bf T}[n] + \hat{\bf V}_{ee}[n] + \int_{\R^3} d{\bf x} \blanky \hat{V}_{\textnormal{ext}}({\bf x}) n({\bf x}) \\
    = F_{HK}[n] + + \int_{\R^3} d{\bf x} \blanky \hat{V}_{\textnormal{ext}}({\bf x}) n({\bf x})
\end{align}

where, in the second line, the first and second terms are both unknown and are independent of the external potential (its sum yields the so-called Hohenberg-Kohn functional), while the third term gives an explicit functional of the density related to the external potential, since it's a one-body operator. Note that the Hohenberg-Kohn functional is universal for all $N$-electrons systems. It's important to note that the Hohenberg-Kohn's theorems are, in essence, uniqueness theorems: the functional of the density, if it exists, then it's unique. The existence of this functional is related to the idea of $v$-representability. \\

A heuristic idea for $v$-representability can be given as follows: consider a "reasonable" density $n$, reasonable in this context meaning: a positively defined density, normalized to the number of electrons present in the system. Then, this begs the question if it is possible to find an external potential that leads to this density, or in other words, given a density profile, is there a system which has this density?
This question was answered by the works of Levy (1982), Lieb (1982) and Englisch \& Englisch (1983), leading to some novel ideas like 

\begin{itemize}
    \item ensemble-$v$-representable, 
    \item a generalization of the Hohenberg-Kohn theorem, which accounts for degenerate groundstates,
    \item and to generalize $v$-representability to $N$-representability. \\
\end{itemize}

\section{Thomas-Fermi Approach}

\section{Auxiliary Systems}

In density functional theory in particular, and in many-body physics, the motivation to consider auxiliary systems is to construct a superset model, which includes the system of interest, without interactions, by adjusting some parameters and obtaining reasonable estimates of physical quantities in a cost-effective way. One way in which to do this is to adjust the external potential, in such a way that the density is easy to calculate. The question is then, for a given material of interacting electrons, is it possible to find an auxiliary system with a modified potential such that they have exactly same density? \\

Consider the interacting Schr\"odinger equation for a many-body system, 

\begin{equation}
    \bigg(-\sum_{i} \frac{\nabla_i^2}{2} + \sum_{i}\hat{V}_{\textnormal{ext}}({\bf x}_i)  + \frac{1}{2} \sum_{i\neq j} \frac{1}{|{\bf x}_i - {\bf x}_j|}\bigg) \Psi({\bf x}_1, \cdots, {\bf x}_n) = E \Psi({\bf x}_1, \cdots, {\bf x}_n),
\end{equation}

which we desire to replace by a non-interacting Schr\"odinger equation for an auxiliary system 

\begin{equation}
    \bigg(-\sum_{i} \frac{\nabla_i^2}{2} + \sum_{i}\hat{V}_{\textnormal{aux}}({\bf x}_i)  \bigg) \Psi_{\textnormal{aux}}({\bf x}_1, \cdots, {\bf x}_n) = E_{\textnormal{aux}} \Psi_{\textnormal{aux}}({\bf x}_1, \cdots, {\bf x}_n),
\end{equation}

with the aim of reproducing the density, given by \eqref{one-body density}

\begin{equation}
{\bf n}({\bf x}) = N \int_{\mathds{R}^{3(N-1)}} \prod_{i=2}^N d{\bf x}_i \prod_{j=2}^{N} \sum_{\sigma_j \in \mathds{C}^2} |\Psi_{\textnormal{aux}}(\tilde{x}_1, \cdots, \tilde{x}_N) |^{2}.
\end{equation}

The auxiliary system's density is easy to calculate, since it is constructed without electron-electron interactions, it is given by 

\begin{align*}
    n({\bf x}) = \sum_{\lambda \in \Lambda}^{\textnormal{occ}} |\phi_{\textnormal{aux}}^{\lambda}({\bf x}) |^2 \textnormal{ with } & \phi_{\textnormal{aux}}^{\lambda} \blanky | \blanky \bigg(- \frac{\nabla^2}{2} + \hat{V}_{\textnormal{aux}}({\bf x}) \bigg) \phi_{\textnormal{aux}}^{\lambda}({\bf x}) = \varepsilon_{\textnormal{aux}}^{\lambda}\phi_{\textnormal{aux}}^{\lambda}({\bf x}) \\
&\Rightarrow \sum_{\lambda \in \Lambda}^{\textnormal{occ}} \bigg[\phi_{\textnormal{aux}}^{\lambda*}({\bf x}) \bigg(-\frac{\nabla^2}{2}\bigg)\phi_{\textnormal{aux}}^{\lambda}({\bf x}) + \hat{V}_{\textnormal{aux}}({\bf x}) |\phi_{\textnormal{aux}}^{\lambda}({\bf x})|^2\bigg] = \sum_{\lambda \in \Lambda}^{\textnormal{occ}} \varepsilon_{\textnormal{aux}}^{\lambda}|\phi_{\textnormal{aux}}^{\lambda}({\bf x})|^2 \\
&\Rightarrow \hat{V}_{\textnormal{aux}}({\bf x}) = \frac{1}{ n({\bf x})} \sum_{\lambda \in \Lambda}^{\textnormal{occ}} \bigg(\varepsilon_{\textnormal{aux}}^{\lambda}|\phi_{\textnormal{aux}}^{\lambda}({\bf x})|^2 + \phi_{\textnormal{aux}}^{\lambda*}({\bf x}) \frac{\nabla^2}{2}\phi_{\textnormal{aux}}^{\lambda}({\bf x}) \bigg),
\end{align*}

note that the problem is self-consistent, since both the eigenvalues and eigenfunctions depend on the external potential. It's not obvious, then, that a solution exists and can be found analytically. Note that this is related to the previous $v-$representability question. 

\section{The Kohn-Sham equations}

The Kohn-Sham equations are based on the idea of auxiliary systems and provide a useful tool to calculate observables in Density Functional Theory. 

\subsection{The exchange correlation term}

Consider the problem of minimizing the density functional to find the groundstate total energy. The total energy functional is given by 

\begin{equation}
 E[n] = \hat{\bf T}[n] + \hat{\bf V}_{ee}[n] + \int_{\R^3} d{\bf x} \blanky \hat{V}_{\textnormal{ext}}({\bf x}) n({\bf x}).
 \label{Hohenberg-Kohn functional}
\end{equation}

Should the functional exist, it is unique. However, the first two terms are unknown. The idea is to combine the use of an auxiliary system to help calculate these two functionals of the density, with a variational approach (eg. Thomas-Fermi, Hartree, etc.) \\

In order to do this, consider equation \eqref{Hohenberg-Kohn functional} and let $T_s[n]$ be the kinetic energy functional of an independent particle system of density $n$. 

\begin{align*}
     E[n] &= \hat{\bf T}[n] + \hat{\bf V}_{ee}[n] + \int_{\R^3} d{\bf x} \blanky \hat{V}_{\textnormal{ext}}({\bf x}) n({\bf x}) \\
     &= \hat{\bf T}[n] + \hat{\bf V}_{ee}[n] + \int_{\R^3} d{\bf x} \blanky \hat{V}_{\textnormal{ext}}({\bf x}) n({\bf x}) + T_s[n]-T_s[n] \\
     &= \hat{\bf T}[n] + \hat{\bf V}_{ee}[n] + \int_{\R^3} d{\bf x} \blanky \hat{V}_{\textnormal{ext}}({\bf x}) n({\bf x}) + T_s[n]-T_s[n] + \frac{1}{2} \int_{\R^6} d{\bf x} d{\bf x'} \blanky \frac{n({\bf x})n({\bf x})}{|{\bf x}-{\bf x'}|} - \frac{1}{2} \int_{\R^6} d{\bf x} d{\bf x'} \blanky \frac{n({\bf x})n({\bf x})}{|{\bf x}-{\bf x'}|} \\
     &= T_s[n] + \int_{\R^3} d{\bf x} \blanky \hat{V}_{\textnormal{ext}}({\bf x}) n({\bf x}) + \frac{1}{2} \int_{\R^6} d{\bf x} d{\bf x'} \blanky \frac{n({\bf x})n({\bf x})}{|{\bf x}-{\bf x'}|} + \underbrace{\hat{\bf T}[n] + \hat{\bf V}_{ee}[n] - T_s[n] - \frac{1}{2} \int_{\R^6} d{\bf x} d{\bf x'} \blanky \frac{n({\bf x})n({\bf x})}{|{\bf x}-{\bf x'}|}}_{\text{$=E_{xc}[n]$}} \\
     &= E^H [n] + E_{xc}[n],
\end{align*}

where on the third line, we added and subtracted the electron-electron energy term in the Hartree approximation, where $E^H[n]$ is the total energy within the Hartree approximation and where $ E_{xc}[n]$ is the exchange-correlation term, which is given by

$$
 E_{xc}[n] = T[n] - T_s[n] + E_{ee}[n] - E^H_{ee} [n].
$$

The exchange-correlation functional is, in essence, a difference between the kinetic energy functional for the real system and the kinetic energy functional for the system of non-interacting particles and the difference between the exact electron-electron interaction functional and its counterpart in the Hartree approximation. \\

\subsection{Derivation of the Kohn-Sham equations}

Now, we will write down one-particle equations using the idea of auxiliary systems. \\

Consider a system of $N$ non-interacting electrons (ie. no electron-electron interaction). Then Schr\"odinger's equation for this system reads

$$
\Psi = \prod_{i=1}^{N} \psi_{i}({\bf x}_i), \blanky \bigg[-\frac{1}{2} \nabla^2 + \hat{V}_{\textnormal{eff}}({\bf x}_i) \bigg] \psi_{i} ({\bf x}) = \lambda_i \psi_{i}({\bf x}_i) \Rightarrow n(\x) = \sum_{i=1}^{N} |\psi_i(\x)|^2.
$$

From the Hohenberg-Kohn theorem we know there is a unique correspondence between the density and the external potential of the system. We also know the total energy as a functional of the density. Now, given a real system with interacting electrons and density $n$, can we find an auxiliary system with non-interacting electrons which the same density? This question is answered by the Kohn-Sham theorem. \\

Consider first the procedure for which we'd find the effective potential, which is unknown at the moment. 
Since densities and external potentials are in unique correspondence, the one-particle wave functions are also unique functionals of the density $\psi_i (\x) = \psi_i ([n], \x)$. To find these functionals, we will use the variational theorem which, in essence, states that the energy functional of the density is stationary around the groundstate density. Then, the best one-particles wave-functions of the auxiliary system which minimize the energy functional can be obtained via a functional derivative, ie.

$$
\frac{\delta(E[n])}{\delta \psi_i} = 0, \textnormal{ under the constraint } \int_{\R^3} d\x \blanky \psi_{i}^{*}(\x) \psi_i(\x) = 1.
$$

with some additional constraints. Therefore we define a set $\{\lambda_i\}_{i=1}^{n}$ of real numbers, the Lagrange multipliers, so as to take into account the normalization constraint. 

\begin{align}
    \frac{\delta}{\delta \psi_{i}^{*}} \bigg[T_s[n] + \int_{\R^3} d{\bf x} \blanky \hat{V}_{\textnormal{ext}}({\bf x}) n({\bf x}) &+ \frac{1}{2} \int_{\R^6} d{\bf x} d{\bf x'} \blanky \frac{n({\bf x})n({\bf x'})}{|{\bf x}-{\bf x'}|} + E_{xc}[n] - \lambda_i \int_{\R^3} d\x \blanky \psi_{i}^{*}(\x) \psi_i(\x)\bigg] = 0 \\
&\Rightarrow \bigg[-\frac{\nabla^2}{2} + \hat{V}_{\textnormal{ext}}({\bf x}) + \underbrace{\int_{\R^3}d\x' \blanky \frac{n({\bf x})}{|{\bf x}-{\bf x'}|}}_{V^H[n]} + \frac{\delta E_{xc}[n]}{\delta n(\x)}\bigg] \psi_i(\x) = \lambda_i \psi_i(\x),
\label{Kohn-Sham equations}
\end{align}

where equation \eqref{Kohn-Sham equations} is called the Kohn-Sham equations, a set of one-particles equations for non-interacting electrons which give the exact electron density $n(\x)$ of the real system. Note that \eqref{Kohn-Sham equations}'s second, third and fourth term on the left hand side, imply that the electrons are submitted to an effective potential given by the sum of said terms. Note that the Kohn-Sham equations are exact, so we can obtain -in principle- the exact density by solving them. However, in practice, this can seldom be performed, since the exchange-correlation term is not known a priori and must be approximated. \\

\subsection{Consequences of the Kohn-Sham equation}

As stated previously, the Kohn-Sham equations are a set of one-particle equations, where the wave-function acts on only one space variable. All independent particles are submitted to an effective potential. By diagonalizing the Kohn-Sham Hamiltonian we obtain the eigenvectors and eigenvalues. \\

Suppose we can solve the Kohn-Sham equations, we have the eigenvectors $\{\psi_{i}(\x)\}_{i=1}^{n}$, from which we can calculate the electron density. Then, using this electron density, we can, in principle, construct any observable, such as the total energy. And via the total energy, a large number of physical observables can be calculated, from equilibrium distances to stability, phonons to defect formation energies and many others. \\

However, how can the Kohn-Sham equations be solved? In principle, by diagonalizing the Kohn-Sham Hamiltonian. This cannot be done unless we specify the exchange-correlation potential and the Hartree potentials, which themselves depend on the electron density. The most common solution is to implement an iterative procedure, in which an output, the density, can be used as an input in the next iteration. This is called the \textbf{self-consistent scheme}. \\

Another very important question concers the interpretation of the Kohn-Sham's eigenvalues. In particular, if they can be interpreted as the energies of the electrons. The electron's energy is defined as the energy to extract the electron from the system and put it infinitely far away. Should this be the case, we could then model the energy levels of the system, if it has a discrete structure, like an atom or a molecule, or, for the case of a solid, build the entire band structure. Unfortunately, this is not the case, since there's no counterpart to the (Hartree-Fock-) Koopmans' theorem for the Kohn-Sham formulation. This is so since the auxiliary system bears no resemblance to the real system, the electronic density notwithstanding. Thus, the eigenvalues are not removal or addition energies. In practice, however, this may not be the case. For the specific case of finite systems, the last occupied eigenvalue has a physical meaning, it is the ionization energy HOMO. \\

The last concern at the moment is, what is the Kohn-Sham potential? It's the sum of the external potential, a known quantity, of the Hartree potential, another known quantity, and of the exchange-correlation potential which, though unknown, is a local, static quantity. 

\subsection{The Kohn-Sham potential}

The Kohn-Sham potential is an effective potential, felt by the non-interacting electrons of the auxiliary system, whose density is the same as that of the real interacting system. This potential is local, it doesn't depend on any particular electron and, in principle, only depends on the electron density 

\section{Local Density Approximation (LDA)}

\subsubsection{The Universal Functional}

The total energy is a functional of the density, which can be written as the sum of a universal functional of the density -which is the same for all systems with the same number of electrons and the same interaction- and a local functional of the density, which depends on the external potential. Due to the Hohenberg-Kohn theorems, the universal functional exists and is unique. \\

The universal functional is given by 

\begin{equation}
    F[n] = T_s[n] + E_{\textnormal{Hartree}}[n] + E_{xc}[n], 
\end{equation}

where the exchange-correlation functional is much smaller than the first two terms though it is vital to explain molecular bonding. There are two main strategies to find approximate exchange-correlation functionals, empirical and non-empirical methodologies. In the empirical approach, a certain functional form is chose, which contains some free parameters, to be adjusted with experimental data-sets. This idea was first introduced by Axel Becke in 1997 (JCP 107, 8554). The first step is to choose variables based on physically relevant ingredients, such as the spin density $n^{\sigma}(\x)$. A simple example is the so-called X$\alpha$-method (Slater 1972) 

$$
V_{xc}^{\sigma}(r) = -6\alpha \bigg[\frac{3}{4\pi} n^{\sigma}(r)\bigg]^{\frac{1}{3}},
$$

where the spin density $n^{\sigma}(\x)$ is thought as the contribution of electrons of spin $\sigma$ to the total electron density, as a function of the position. Other common building blocks include the gradient density $|\nabla n^{\sigma}(\x)|$ and the kinetic-energy density $\tau^{\sigma}(\x) = \sum_{i}^{N_\sigma} |\nabla \psi_i^\sigma(\x)|^2$, which is the sum over the gradient squares of the occupied Kohn-Sham orbitals. The next step is to define the exchange-correlation functional as some power series in the chosen series, 

$$
f[x,y] = \sum_{ij}^{MM'} c_{ij} x^i y^i,
$$

which introduces a set of expansion parameters, $c_ij$, which can then be fitted by least-squares on empirical datasets. The most famous empirical dataset is the G2 data set (John Pople et. al, 1991-1998), which has 148 formation enthalpies, 88 ionization potentials, 58 electron affinities and 8 proton affinities of different molecules. Another methodology includes using Machine Learning techniques to bypass the Kohn-Sham equations entirely, and directly approximate the potential density map and the map from density to total energy (F. Brockherde, L. Vogt, L. Li, Me. Tuckerman, K. Burke, and K.-R M\"uller, Nature Comm. 8, 872 (2017)). \\

A non-empirical approximation of the exchange-correlation functional is constructed from exact constraints or from a derivation, which doesn't contain any fitting parameters. \\

\subsection{Local Density Approximations and Jacob's Ladder of Approximations}

It turns out that there is a systematic way of designing exchange-correlation functionals, but improving performance requires increasingly complexity. The Jacob's Ladder (formulated by John Perdew, 2001) is a classification scheme of various approximations for the exchange-correlation functional. On the other hand, the local density approximation (LDA) is the basis and point of departure for constructing many modern functionals. The zero-order approximation consists of neglecting the exchange-correlation term in its entirety. The first-order approximation are the LDA, the second order are the GGA, followed by the meta-GGA, Hybrid and, finally, RPA-like models, with each step increasing the calculations' computational cost but with better performing results. \\

\paragraph{\textbf{Local density Approximations}}

The Local Density Approximation is the lowest rung of Jacob's ladder, which is an example of a non-empirical exchange-correlation functional and uses a reference system, which provides the constraint to construct the functional. \\

To define the reference system, consider an array of ionic cores in a solid, embedded in a uniform sea of delocalized valence electrons. An even simpler picture of a solid consists of positive cores uniformly smeared out on a surface (sometimes called a jellium) and a homogeneous electron gas of the same density $\Bar{n}$ as the jellium, such that the system has neutral charge. In this case, our reference system is the homogeneous electron gas and the only variable of interest is the uniform density, $\Bar{n}$, with the energy per volume being

$$
\epsilon^{\textnormal{hom}}(\Bar{n}) = \frac{E^{\textnormal{hom}}}{V},
$$

ie. only being a function, not a functional, of the uniform density. The energy density can then be written as a sum of the kinetic energy density and the exchange-correlation density, ie. 

$$
\epsilon^{\textnormal{hom}} = \epsilon_{\textnormal{kin}}^{\textnormal{hom}} + \epsilon_{\textnormal{xc}}^{\textnormal{hom}}, \textnormal{ where } \begin{array}{c}
     \epsilon_{\textnormal{kin}}^{\textnormal{hom}} = \frac{(3\pi^2 \Bar{n})^{\frac{5}{3}}}{10\pi^2}, \\
     \\
     \epsilon_{\textnormal{xc}}^{\textnormal{hom}} = \frac{(3\pi^2 \Bar{n})^{\frac{4}{3}}}{4\pi^3} + \epsilon_{c}^{\textnormal{hom}}.
\end{array}
$$

note that in the homogeneous electron gas, the exchange term is known but there is no known analytic formula for the correlation term (though there are well known formulas based on Quantum Monte-Carlo). \\

Then, in the LDA approximation, the exchange-correlation energy functional is given as an integral over the exchange-correlation energy density of the homogeneous electron gas, evaluated at the density at each point $\x$, ie. 

\begin{equation}
E_{xc}^{\textnormal{LDA}} [n] = \int_{\R^3} d^3 \x \blanky \epsilon_{\textnormal{xc}}^{\textnormal{hom}}(n(\x)).
\label{LDA exchange correlation}
\end{equation}

\blanky \\

\subsection{Exact Constraints and GGAs}

Physically, consider a very strongly inhomogeneous system with an electron cloud resembling spherical shells. A small volume element at position $\x_i$ as a small piece of a homogeneous electron gas with the same density as in the real system at that point and at a different position, there will be a different density. Therefore, the LDA consists in locally treating the real system as a homogeneous electron gas, with the exchange correlation energy given by equation \eqref{LDA exchange correlation}. \\

LDA is a very useful techniques, giving, for example, total energies of molecules within $1\% - 5\%$ of the exact results and gives lattice constants of solids within $1\% - 2\%$ of the exact results. However, the LDA produces atomization energies too big (overbinding), there are no bounded negative ions and not accurate enough for chemical applications. \\

Consider an electron in a many-body interacting electron system. There are three mechanisms whereby electron "avoid" each other:

\begin{itemize}
    \item classical Coulomb interaction, 
    \item the Pauli principle, 
    \item non-classical correlation effects, which can occur between parallel and anti-parallel spins. 
\end{itemize}

Then, the \textit{exchange-correlation hole} is the region around each electron where it's less likely to find another electron due to exchange-correlation effects. In mathematical terms, we define a pair density as 

\begin{equation}
    P(\x, \x') = N(N-1) \int_{\Omega \subset \R^{3(N-2)}} \prod_{i=3}^{N} d^{3}\x_i \prod_{j=3}^{N} \sum_{\sigma_j \in \mathds{C}^2} |\Psi(\x, \x', \tilde{x}^{(3)}\cdots, \tilde{x}^{(N)})|^2,
    \label{Pair density}
\end{equation}

ie. the probability of finding any pair of electrons in the volume elements $d\x$ and $d\x'$. Note that equation \eqref{Pair density} includes a summation over the spins of all other electrons. If the electrons were classical particles, then the pair density would simply be the product of the one-particle densities,

$$
P(\x, \x')_{\textnormal{cl}} = n(\x)n(\x').
$$

But for real, interacting, quantum electrons we have that 

$$
P(\x, \x')_{\textnormal{cl}} = n(\x)n(\x') + n(\x) n_{xc}(\x, \x'),
$$

where $n_{xc}(\x, \x')$ is the exchange-correlation hole density. This term accounts for the exchange and correlation quantum effects whereby electrons avoid one another. Now, it's possible to write the exchange correlation hole as the sum of an exchange hole and a correlation hole 

$$
n_{xc}(\x, \x') = n_{x}(\x, \x') + n_{c}(\x, \x'), \textnormal{ which satisfy} \begin{array}{c}
     n_{x}(\x, \x') \leq 0  \\
     \blanky \\
     \int_{\R^{3}} d\x' \blanky n_{x}(\x, \x') = -1\\
     \blanky \\
     \int_{\R^{3}} d\x' \blanky n_{c}(\x, \x') = 0.
\end{array}
$$

The LDA exchange-correlation hole $n_{xc}^{\textnormal{LDA}}(\x,\x')$ satisfies these constraints, ie.

$$
\begin{array}{c}
     n_{xc}^{\textnormal{LDA}}(\x,\x') \leq 0  \\
     \blanky \\
     \int_{\R^{3}} d\x' \blanky n_{xc}^{\textnormal{LDA}}(\x,\x') = -1\\
     \blanky \\
     \int_{\R^{3}} d\x' \blanky n_{xc}^{\textnormal{LDA}}(\x,\x') = 0.
\end{array}
$$

The physical reason to impose such constraint is that our reference system, the homogeneous electron gas, satisfies these constraints as well. Following this line of thought, the exchange-correlation energy functional can be written as 

\begin{equation}
    E_{xc} [n] = \int_{\R^6} d\x d\x' \blanky \frac{n(\x) \bar{n}_{xc}(\x, \x')}{|\x - \x'|},
    \label{exchange-correlation energy}
\end{equation}

which resembles the Hartree energy but including the averaged exchange-correlation hole coupling constant, $\bar n_{xc}$. It turns out that the LDA exchange-correlation hole itself is not a good approximation of the exact exchange-correlation hole density, but the theory's overall accuracy improves since many imperfections are integrated out. \\


In order to improve, one needs to find better approximations for the exchange-correlation hole, which means going beyond the local density as the main quantity. Eg., consider a strongly inhomogeneouss density distribution. Kohn's concept of nearsightedness postulates that $n_{xc}(\x, \x')$ is mostly determined by $n(\x)$, thus only needing input information on the density, within a small neighbourhood of the reference position (the semi-local approximation). The idea is then, to construct an exchange-correlation functional in terms of the local density and its gradient. This is the main idea behind the \textbf{Generalized Gradient Approximation (GGA)}, where the exchange correlation energy functional can then be written as 


\begin{equation}
    E_{xc}^{\textnormal{GGA}}[n] = \int_{\R^3} \blanky \epsilon_{xc}^{\textnormal{hom}}(n(\x)) F_{xc} (n^{\sigma}(\x), \nabla n^{\sigma}(\x)),
\end{equation}

where $F_{xc}(\cdot)$ is a generic functional, the enhancement factor, which can then be determined by exact constraints. Therefore, GGAs can then be used to determine the exchange-correlation energy functional without empirical parameters, eg. the PBE-functional (J.P. Perdew, K. Burke, and M. Ernzerhof, PRL 79, 3865, (1996)) or the BLYP-functional (Becke-Lee-Yang-Parr 1988). GGAs significantly improve accuracy without a significant increase in computational complexity. \\

\subsection{Meta-GGAs}

There are approximations which go beyond the LDA and GGAs, the so-called Meta-GGAs, pioneered by J. Purdew. Consider some unknown electronic density distribution, for which we desire to write approximations in order to calculate the exchange correlation energy. As we've seen, LDA deals with local functionals, which only depends on the density and/or orbitals and their gradients at the reference position; needing only local information to construct the density. In the local and semilocal functionals, on the other hand, the exchange-correlation energy density depends on the density. In non-local functionals, the exchange-correlation energy density at point $\x$, depends on the density and/or the orbitals everywhere throughout the system. \\

Meta-GGAs consider semi-local functionals, Hybrid and RPA-like models consider fully non-local functionals. In the Meta-GGAs' case, the exchange-correlation energy is written as 

\begin{equation}
    E_{xc}^{\textnormal{MGGA}}[n^\sigma] = \int_{\R^3} d\x \blanky f(n^\sigma, \nabla n^\sigma, \tau^\sigma),
\end{equation}

where

$$\tau^\sigma(\x) = \frac{1}{2} \sum_{j}^{\textnormal{occ}} |\nabla\psi_j^\sigma (\x)|, \blanky \sigma \in \mathds{C}^2,$$

is the kinetic energy density, an implicit semi-local functional. Well known examples of Meta-GGAs are the TPSS-method (Tao-Perdew-Staroveov-Scuseria 2003) and the SCAN-functional ("Strongly Constrained and Appropriatley Normed", Sun-Ruszinsky, Perdew 2016). The physical reason to include the kinetic energy density is because said density is highly sensitive to the electron's degree of localization, helping the functional to better model covalent single bonds and metallic bonds. Furthermore, this density incorporates exact constraints of one and two-electron densities. Since the kinetic energy density is an implicit functional, there is no analytic expression for it in terms of the density. Naturally, the Hohenberg-Kohn theorem establishes that this must be a functional of the density, since the Kohn-Sham orbitals depend on the density. \\

In Y. Zhang et al., (NPJ Comput. Mater. 4, 9 (2018)), experimental enthalpies of solids (main group binary compounds) are compared with PBE-based and SCAN-based theoretical results. The SCAN-Meta-GGA approach yields a mean absolute error (MEA) close to chemical accuracy (0.038 eV per atom) while the PBE's MEA is close to 0.293 eV per atom. The study found that the results were less accurate for transition metal compounds but still very good. In J. Sun et al., Nature Chem. 8, 831, (2016), structures formed by six water molecules, SCAN outperformed both the hybrid PBE mode, PBE0, as well as the computationally-expensive CCSDT, a high-level Quantum Chemistry, and predicted the correct energetic ordering of four water hexagrems. It turns out that Meta-GGAs method do not offer significantly more accurate results that LDAs when the exchange-correlation hole is sufficiently localized. Meta-GGAs method tend to fail when studying weakly bonded systems, where long-range van der Waals forces are important, as well for strongly-correlated systems, and for the band-gap calculations. \\

\subsection{Hybrids and RPA-Like}

Hybrid models are based on a mixture of a semi-local exchange correlation functional and a non-local exact exchange functional and were first proposed by Axel Becke (1993). In this approach, the exchange-correlation energy is written as \eqref{exchange-correlation energy} where 

\begin{equation}
    \bar{n}_{xc}(\x, \x') = \int_{\R_{[0,1]}} d\lambda \blanky n_{xc}^\lambda(\x, \x'), 
    \label{coupling constant}
\end{equation}

is the coupling constant averaged exchange correlation hole density. The most natural approach is to consider \eqref{coupling constant} as a linear interpolation, where

\begin{equation}
    \bar{n}_{xc} \approx \frac{1}{2} n_{xc}^{\lambda = 0} + \frac{1}{2} n_{xc}^{\lambda = 1}.
\end{equation}

Physically, $\lambda$ is a coupling constant ie. a measure of the correlation strength: $\lambda = 0$ there's no correlation and describes exact exchange

$$
n_{xc}^{\lambda = 0}(\x, \x') = n_{x}(\x, \x').
$$

The exact exchange energy is given by a Fock-exchange expression

\begin{equation}
    E_x^{\textnormal{exact}}[n] = -\frac{1}{2} \sum_{\sigma \in \mathds{C}^2} \sum_{ij}^{\textnormal{occ}} \int_{\R^6} d\x d\x' \blanky \frac{\psi_{i}^{\sigma {*}} (\x) \psi_{i}^{\sigma} (\x') \psi_{j}^{\sigma} (\x) \psi_{j}^{\sigma {*}} (\x')}{|\x - \x'|}.
    \label{exact exchange}
\end{equation}

Note that \eqref{exact exchange} formally resembles the Hartree-Fock exchange energy but in \eqref{exact exchange}, the sum is evaluated at the Kohn-Sham orbitals, which themselves depend on the density. Thus, the exact exchange energy is an implicit density functional. The dependence of said exact exchange energy on the orbital is explicit and non-local. Therefore, a first guess for a hybrid functional consists on mixing the exact exchange with the exchange correlation energy at the LDA level, as an approximation for the contribution at $(\lambda=1)$-order,

\begin{equation}
    E_{xc}^{\textnormal{hybrid}} = \frac{1}{2}    E_{x}^{\textnormal{exact}} + \frac{1}{2}    E_{xc}^{\textnormal{LDA}}.
    \label{naive hybrid}
\end{equation}

Unfortunately, this n\"aive hybrid approximation is not useful in practice and doesn't improve accuracy. However, there are several techniques to improve the interpolation schemes or the coupling constant integration. One such successful workaround is PBE0 (1999) where the exchange-correlation energy is approximated as 

$$
    E_{xc}^{\textnormal{hybrid}} = \frac{1}{4}    E_{x}^{\textnormal{exact}} + \frac{3}{4}    E_{x}^{\textnormal{GGA}} + E_{c}^{\textnormal{GGA}}.
$$

Another such example is BELYP (1994), where now the approximation contains three parameters, $a, b, c$, and mixes LDA and GGA exchange and correlation with exact exchange, 

$$
    E_{xc}^{\textnormal{hybrid}} = (1-a)    E_{x}^{\textnormal{LDA}} + aE_{x}^{\textnormal{exact}} + bE_{x}^{\textnormal{BB8}} + cE_{c}^{\textnormal{LYP}} + (1-c)E_{c}^{\textnormal{LDA}},
$$

where the values of $a,b,c$ can then be fitted with experimental datasets. \\

The main problem with hybrid and subsequent approximations is their increased computational complexity, compared to the LDA and GGAs counterparts. In the LDA and GGA, the density functionals are explicit, thus the exchange-correlation potential is 

$$
V_{xc}(\x) = \frac{\delta E_{xc}[n]}{\delta n(\x)}.
$$

The same cannot be said for the Meta-GGAs and Hybrid functionals. Said functionals depend on the orbitals, and thus are implicit. The functional derivative is now much more difficult to obtain directly, thus increasing the problem's computational complexity. Note that the exchange correlation potential can be re-written as 

\begin{align}
V_{xc}(\x) = \frac{\delta E_{xc}[n]}{\delta n(\x)} = \sum_{i} \int_{\R^3} d\x' \frac{\delta E_{xc}[n]}{\delta \psi_i (\x')}\frac{\delta \psi_i (\x')}{\delta n(\x)}.
\label{Generalized Kohn-Sham}
\end{align}

\eqref{Generalized Kohn-Sham}'s first multiplicative term, $ \frac{\delta E_{xc}[n]}{\delta \psi_i (\x')}$, can be obtained since we know $E_{xc}[n]$ explicitly. In respect to the second functional derivative, we can calculate functional derivative, $\frac{\delta n(\x)}{\delta \psi_i (\x')}$ though not the one which appears in \eqref{Generalized Kohn-Sham} since the orbitals, $\psi_i(\x)$, implicitly depend on the density via the self-consistent Kohn-Sham equation. Therefore, in many cases, the Generalized Kohn-Sham approach is implemented, where one simply computes the variation of the energy with respect to the orbitals ie. 

$$
V_{xc, i}^{\textnormal{non-local}} (\x) = \frac{\delta E_{xc}[n]}{\delta \psi_i (\x')},
$$

which gives a non-local exchange correlation potential and therefore, said potential, can be used in the traditional Kohn-Sham equation. This gives the Generalized Kohn-Sham equations, which are now non-local. For the hybrid functionals with exact exchange, this is similar to the Hartree-Fock approach. \\

Some of the main takeaways are: 

\begin{itemize}
    \item Hybrid functionals perform very well for reproducing molecules' energetics, good band-gaps and give good geometries \& lattice constants, 
    \item but GGAs and Meta-GGAs often perform better
    \item Hartree-Fock exchange can be expensive for solids, depending on the implementation. Thus Hybrid functionals are not widely applicable for the study of solids. 
\end{itemize}

\subsection{Band-gaps of solids}

\end{document}
